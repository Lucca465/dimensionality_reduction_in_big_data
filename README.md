# dimensionality_reduction_in_big_data
Devido a revolução tecnológica, mais dados estão sendo gerados no
dia a dia de forma exponencial, causando problemas de processamento e de
armazenamento. Por este motivo, este projeto de TCC busca por meio dos
algoritmos PCA, Kernel PCA e LDA reduzir a dimensionalidade dos dados
solucionar este problema. O objetivo é comparar suas capacidades de redução
de dimensionalidade em bases de dados estruturada e fazer uma análise na
prática do tempo de processamento e, utilizando o RandomForest, relatar a taxa
de acerto da previsão dos dados. A abordagem teórico-metodológica envolve
revisão literária, análise matemática e estudo de aplicações. Também com o
objetivo de comparar a capacidades de redução de dimensionalidade, vamos
utilizar o PCA em uma base de dados não estruturada composta por 200
imagens. As comparações foram realizadas analisando a capacidade de
preservação da qualidade, tempo de processamento e economia de
armazenamento. Foram mostrados resultados que permitiram comparar as
técnicas em termos de preservação de informações e tempo de processamento.
Experimentos com conjuntos de dados e métricas de avaliação foram
realizados. Os resultados forneceram percepções valiosos para aplicação
prática em análise de dados multivariados.
